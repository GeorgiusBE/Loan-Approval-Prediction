{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7b36fe-ea85-4253-a35e-62c415cafd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold, train_test_split, StratifiedKFold, cross_val_predict, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, OrdinalEncoder\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, average_precision_score, f1_score\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "import optuna\n",
    "from optuna.integration import XGBoostPruningCallback\n",
    "from optuna.integration import LightGBMPruningCallback\n",
    "from optuna.integration import CatBoostPruningCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55367ecc-2d66-45e7-a29e-397bab9771cb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Import Loan Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ff041c-ed53-4ee7-939e-0416d140ca71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset\n",
    "df_train = pd.read_csv(\"train.csv\")\n",
    "df_test = pd.read_csv(\"test.csv\") # The testing dataset does not contain the target variable \"loan_status\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44afc43-2ee9-4170-9d4a-fc74cf601ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# info on training dataset\n",
    "print(df_train.columns, '\\n')\n",
    "print(df_train.shape)\n",
    "display(df_train.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e745e7-75f2-4dd4-a23d-01f6953e6105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# info on testing dataset\n",
    "print(df_test.columns, '\\n')\n",
    "print(df_test.shape)\n",
    "display(df_test.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767f9676-4141-4817-a5c8-5807133ab23d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Cleaning Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379cabdd-542e-442c-96f9-554024560f95",
   "metadata": {},
   "source": [
    "## Checking for Nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d7e1e3-4891-4af9-b698-724a0f6b6a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c398eaf-fe3d-449a-9010-4dc46a6f56a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea413ef8-80ea-464c-9e9d-878b1e1e4a3b",
   "metadata": {},
   "source": [
    "## Remove unnecessary features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721cd6ea-52b3-42a4-8af2-7581b358d899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'id' column\n",
    "df_train.drop('id', axis=1, inplace=True)\n",
    "df_test.drop('id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823148a3-949e-406b-891c-ffc48a151a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfc48f1-d392-4840-8b9c-a583f40039c6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# EDA: Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561545c2-c928-493b-a6d9-e2f2869752fb",
   "metadata": {},
   "source": [
    "Fields in the dataset:\n",
    "- `person_age`\n",
    "    - The age of the borrower\n",
    "- `person_income`\n",
    "    - The annual income of the borrower\n",
    "- `person_home_ownership`\n",
    "    - The home ownership status of the borrower\n",
    "- `person_emp_length`\n",
    "    - How long (in years) the borrower has been in employment\n",
    "- `loan_intent`\n",
    "    - The borrower's intended use of the loan\n",
    "- `loan_grade`\n",
    "    - The loan grade, measuring the loan default rate\n",
    "- `loan_amnt`\n",
    "    - Amount borrowed by the borrower\n",
    "- `loan_int_rate`\n",
    "    - Loan interest rate\n",
    "- `loan_percent_income`\n",
    "    - The ratio between `loan_amnt` and `person_income`\n",
    "- `cb_person_default_on_file`\n",
    "    - The borrower's prior default status\n",
    "- `cb_person_cred_hist_length`\n",
    "    - The length of the borrower's credit history\n",
    "- `loan_status`\n",
    "    - This is the target variable\n",
    "    - `0` indicates non-default; `1` indicates default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde9e7de-2b96-4e1a-8406-7a8fc03f8aca",
   "metadata": {},
   "source": [
    "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "EDA will be performed in Tableau\n",
    "- Refer to https://www.kaggle.com/code/satyaprakashshukl/loan-approval-prediction/notebook, to give ideas on what charts to be generated\n",
    "\n",
    "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6c7839-9596-4385-9043-f4cd6c482bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the data type of each field\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef6318c-a8f9-40c9-beb3-105466b7f4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for data imbalance\n",
    "df_train['loan_status'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7b4457-49b7-4ed3-a326-0b062590c266",
   "metadata": {},
   "source": [
    "The dataset is very imbalanced! This will be addressed later by applying class weights and threshold tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc8da75-f944-423e-8e2d-60f89bcc5f83",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Feature Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bcdf3d-95c0-466f-a4bc-b98aeab62962",
   "metadata": {},
   "source": [
    "- One-hot-encoding -> Nomical categorical features\n",
    "    - person_home_ownership, loan_intent, cb_person_default_on_file\n",
    "- Ordinal Encoder -> Ordinal categorical features\n",
    "    - loan_grade\n",
    "- Robust Scaling -> Numerical features\n",
    "    - person_age, person_income, person_emp_length, loan_amnt, loan_int_rate, loan_percent_income, cb_person_cred_hist_length\n",
    "- Target variable\n",
    "    - loan_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adc256d-4926-454e-a7da-56f01405a05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_trans = df_train.copy()\n",
    "df_test_trans = df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e4aac2-56e2-4fb9-8fb1-2dcd700af437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_scaling(df_train, df_test, feature_list):\n",
    "    '''\n",
    "    Use Robust Scaler to scale numerical features\n",
    "    \n",
    "    Input\n",
    "    -----\n",
    "    - df: DataFrame that stores all features\n",
    "    - feature_list: List of features in df being transformed\n",
    "    \n",
    "    Output\n",
    "    ------\n",
    "    A subset of the orignial df, containing only the transformed features\n",
    "    '''\n",
    "    \n",
    "    scaler = RobustScaler()\n",
    "    scaled_df_train = scaler.fit_transform(df_train[feature_list])\n",
    "    scaled_df_test = scaler.transform(df_test[feature_list])\n",
    "    \n",
    "    return scaled_df_train, scaled_df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b787ef72-c635-485a-b562-897d1d60af3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform robust scaling on numerical features\n",
    "num_features = [\n",
    "    'person_age', 'person_income', 'person_emp_length', 'loan_amnt', 'loan_int_rate', 'loan_percent_income', 'cb_person_cred_hist_length'\n",
    "]\n",
    "\n",
    "df_train_trans[num_features], df_test_trans[num_features] = robust_scaling(df_train, df_test, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3070c9-831e-481a-930a-b40440c9f066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform one-hot encoding on nominal cateogrical features\n",
    "nom_cat_features = [\n",
    "    'person_home_ownership', 'loan_intent', 'cb_person_default_on_file'\n",
    "]\n",
    "\n",
    "df_train_trans = pd.get_dummies(df_train_trans, columns=nom_cat_features)\n",
    "df_test_trans = pd.get_dummies(df_test_trans, columns=nom_cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a460350-a366-4ec5-aecf-a3d81f64e9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform ordinal encoding on ordinal cateogrical features\n",
    "ord_cat_features = [\n",
    "    'loan_grade'\n",
    "]\n",
    "\n",
    "ord_encoder = OrdinalEncoder(categories=[\n",
    "    ['A', 'B', 'C', 'D', 'E', 'F', 'G']\n",
    "])\n",
    "\n",
    "df_train_trans[ord_cat_features] = ord_encoder.fit_transform(df_train[ord_cat_features])\n",
    "df_test_trans[ord_cat_features] = ord_encoder.transform(df_test[ord_cat_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5c51a9-dafe-4118-9be3-37999689012c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target variables\n",
    "X_train, y_train = df_train_trans.drop('loan_status', axis=1), df_train_trans['loan_status']\n",
    "X_test = df_test_trans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f898652-b506-4706-8650-91851028e059",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8228465f-bcdd-4284-a998-8a5486269a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThresholdTunedClassifier():\n",
    "    \"\"\"\n",
    "    A meta-estimator that:\n",
    "      1) tunes hyperparameters via inner CV (RandomizedSearchCV, scoring=AP by default),\n",
    "      2) produces OOF probabilities via outer CV,\n",
    "      3) (optionally) learns a calibration mapping on OOF probs (isotonic or Platt),\n",
    "      4) selects the F1-optimal decision threshold on (calibrated or raw) OOF probs,\n",
    "      5) refits the best model on the full training set, and\n",
    "      6) predicts with the frozen threshold.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        random_state=123,\n",
    "        n_jobs=-1,\n",
    "        verbose=0,\n",
    "    ):\n",
    "        self.random_state = random_state\n",
    "        self.n_jobs = n_jobs\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # Learned attributes after fit()/tuning\n",
    "        self.best_estimator_ = None\n",
    "        self.calibrator_ = None\n",
    "        self.use_calibration_ = False\n",
    "        self.threshold_ = 0.5\n",
    "\n",
    "    # ------------ helpers ------------\n",
    "\n",
    "    @staticmethod\n",
    "    def _max_f1_threshold(y_true, y_proba):\n",
    "        \"\"\"\n",
    "        Find decision threshold that maximizes F1.\n",
    "\n",
    "        Note: precision_recall_curve returns precision/recall length (n_thr+1),\n",
    "        thresholds length (n_thr). Align F1 to thresholds using f1[:-1].\n",
    "        \"\"\"\n",
    "        prec, rec, thr = precision_recall_curve(y_true, y_proba)\n",
    "        f1 = 2 * prec * rec / (prec + rec + 1e-12)\n",
    "\n",
    "        if thr.size == 0:\n",
    "            # degenerate case (e.g., constant scores); fall back to 0.5\n",
    "            return 0.5, float(f1_score(y_true, (y_proba >= 0.5).astype(int)))\n",
    "\n",
    "        idx = int(np.nanargmax(f1[:-1]))     # align with thresholds\n",
    "        optimal_thr = float(thr[idx])\n",
    "        optimal_f1  = float(f1[:-1][idx])\n",
    "        return optimal_thr, optimal_f1\n",
    "\n",
    "    def _fit_calibrator(self, y_proba, y_true):\n",
    "        \"\"\"\n",
    "        Fit probability calibrator (isotonic or Platt).\n",
    "        \"\"\"\n",
    "        if self.calibration_method == \"isotonic\":\n",
    "            iso = IsotonicRegression(out_of_bounds=\"clip\")\n",
    "            iso.fit(y_proba, y_true)\n",
    "            return iso\n",
    "        elif self.calibration_method == \"platt\":\n",
    "            lr = LogisticRegression(solver=\"lbfgs\", max_iter=1000)\n",
    "            lr.fit(y_proba.reshape(-1, 1), y_true)  # requires 2D\n",
    "            return lr\n",
    "        else:\n",
    "            raise ValueError(\"calibration_method must be 'isotonic' or 'platt'\")\n",
    "\n",
    "    def _apply_calibrator(self, calibrator, y_proba):\n",
    "        \"\"\"\n",
    "        Apply fitted calibrator to raw probabilities.\n",
    "        \"\"\"\n",
    "        if calibrator is None:\n",
    "            return y_proba\n",
    "        if isinstance(calibrator, IsotonicRegression):\n",
    "            return calibrator.transform(y_proba)\n",
    "        elif isinstance(calibrator, LogisticRegression):\n",
    "            return calibrator.predict_proba(y_proba.reshape(-1, 1))[:, 1]\n",
    "        else:\n",
    "            raise TypeError(\"Unknown calibrator type\")\n",
    "\n",
    "    # ------------ core API ------------\n",
    "\n",
    "    def model_tuning(self, X, y, base_estimator, param_distributions, n_iter=1, n_folds=3, tuner_scoring=\"average_precision\"):\n",
    "        \"\"\"\n",
    "        Inner-CV hyperparameter tuning. Sets self.best_estimator_ and returns it.\n",
    "        \"\"\"\n",
    "        self.base_estimator = base_estimator\n",
    "        self.param_distributions = param_distributions\n",
    "        self.n_iter = n_iter\n",
    "        self.tuner_scoring = tuner_scoring\n",
    "\n",
    "        cv = StratifiedKFold(\n",
    "            n_splits=n_folds, shuffle=True, random_state=self.random_state\n",
    "        )\n",
    "\n",
    "        tuner = RandomizedSearchCV(\n",
    "            estimator=self.base_estimator,\n",
    "            param_distributions=self.param_distributions,\n",
    "            n_iter=self.n_iter,\n",
    "            scoring=self.tuner_scoring,\n",
    "            cv=cv,\n",
    "            n_jobs=self.n_jobs,\n",
    "            random_state=self.random_state,\n",
    "            verbose=self.verbose,\n",
    "            refit=True,\n",
    "        )\n",
    "        tuner.fit(X, y)\n",
    "        self.best_estimator_ = tuner.best_estimator_\n",
    "        return self.best_estimator_\n",
    "    \n",
    "    def enter_best_model_config(self, best_model):\n",
    "        \"\"\"\n",
    "        Manually define the hyper-parameter configuration for the best model\n",
    "        \"\"\"\n",
    "        self.best_estimator_ = best_model\n",
    "        return self.best_estimator_\n",
    "    \n",
    "    def decision_thr_tuning(self, X, y, n_folds=3):\n",
    "        \"\"\"\n",
    "        Build OOF probabilities (using OUTER CV), compute F1-opt threshold,\n",
    "        and return (oof_proba, thr_raw, f1_raw).\n",
    "        \"\"\"\n",
    "        cv = StratifiedKFold(\n",
    "            n_splits=n_folds, shuffle=True, random_state=self.random_state\n",
    "        )\n",
    "\n",
    "        oof_proba = cross_val_predict(\n",
    "            self.best_estimator_, X, y, cv=cv, method=\"predict_proba\", n_jobs=self.n_jobs\n",
    "        )[:, 1]\n",
    "\n",
    "        thr_raw, f1_raw = self._max_f1_threshold(y, oof_proba)\n",
    "        self.threshold_ = thr_raw\n",
    "        return oof_proba, thr_raw, f1_raw\n",
    "\n",
    "    def calibrate_prob(self, y, oof_proba, thr_raw, f1_raw, calibration_method=\"isotonic\"):\n",
    "        \"\"\"\n",
    "        Calibration learned on OOF probs.\n",
    "        \"\"\"\n",
    "        self.calibration_method = calibration_method\n",
    "\n",
    "        calibrator = self._fit_calibrator(oof_proba, y)\n",
    "        oof_proba_cal = self._apply_calibrator(calibrator, oof_proba)\n",
    "        thr_cal, f1_cal = self._max_f1_threshold(y, oof_proba_cal)\n",
    "\n",
    "        use_cal = f1_cal > f1_raw\n",
    "\n",
    "        if use_cal:\n",
    "            self.use_calibration_ = True\n",
    "            self.calibrator_ = calibrator\n",
    "            self.threshold_ = thr_cal\n",
    "        else:\n",
    "            self.use_calibration_ = False\n",
    "            self.calibrator_ = None\n",
    "            self.threshold_ = thr_raw\n",
    "\n",
    "        return self.threshold_\n",
    "\n",
    "    def model_training(self, X, y):\n",
    "        if self.best_estimator_ is None:\n",
    "            raise RuntimeError(\"Call model_tuning() before model_training().\")\n",
    "        self.best_estimator_.fit(X, y)\n",
    "\n",
    "    def generate_oof_proba_preds(self, X, y, n_folds):\n",
    "        # Compute OOF probabilities\n",
    "        cv = StratifiedKFold(\n",
    "            n_splits=n_folds, shuffle=True, random_state=self.random_state\n",
    "        )\n",
    "        y_proba_pred = cross_val_predict(\n",
    "            self.best_estimator_, X, y, cv=cv, method=\"predict_proba\", n_jobs=self.n_jobs\n",
    "        )[:, 1]\n",
    "        \n",
    "        return y_proba_pred\n",
    "    \n",
    "    def analyze_performance(self, X, y, n_folds):\n",
    "        # Compute OOF probabilities\n",
    "        y_proba_pred = self.generate_oof_proba_preds(X, y, n_folds)\n",
    "        \n",
    "        # Apply calibration (if enabled)\n",
    "        if self.use_calibration_:\n",
    "            y_proba_eval = self._apply_calibrator(self.calibrator, y_proba_pred)\n",
    "        else:\n",
    "            y_proba_eval = y_proba_pred\n",
    "        y_proba_eval = np.clip(y_proba_eval, 0.0, 1.0)\n",
    "        \n",
    "        # Metrics independent on Decision Threshold\n",
    "        roc_auc = roc_auc_score(y, y_proba_pred)\n",
    "        average_precision = average_precision_score(y, y_proba_pred)\n",
    "        \n",
    "        # Metrics dependent on Decision Threshold\n",
    "        y_pred = (y_proba_eval >= self.threshold_).astype(int)\n",
    "        f1_pos = f1_score(y, y_pred, pos_label=1, average=\"binary\")  # class 1\n",
    "        f1_neg = f1_score(y, y_pred, pos_label=0, average=\"binary\")  # class 0\n",
    "        f1_macro = f1_score(y, y_pred, average=\"macro\")\n",
    "        \n",
    "        results = {\n",
    "            \"roc_auc\": float(roc_auc),\n",
    "            \"average_precision\": float(average_precision),\n",
    "            \"f1_class_1\": float(f1_pos),\n",
    "            \"f1_class_0\": float(f1_neg),\n",
    "            \"f1_macro\": float(f1_macro),\n",
    "        }\n",
    "        self.last_cv_metrics_ = results\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Predict class probabilities; applies calibration if enabled.\n",
    "        Returns shape (n_samples, 2) = [P(class 0), P(class 1)].\n",
    "        \"\"\"\n",
    "        if self.best_estimator_ is None:\n",
    "            raise RuntimeError(\"Call model_training() before predict_proba().\")\n",
    "        proba = self.best_estimator_.predict_proba(X)[:, 1]\n",
    "        if self.use_calibration_:\n",
    "            proba = self._apply_calibrator(self.calibrator_, proba)\n",
    "        proba = np.clip(proba, 0.0, 1.0)\n",
    "        return np.vstack([1 - proba, proba]).T\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Thresholded class prediction using self.threshold_.\n",
    "        \"\"\"\n",
    "        p = self.predict_proba(X)[:, 1]\n",
    "        return (p >= self.threshold_).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c5e017-759b-4e87-9c0c-a96e5fd52ecb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d296a19-3e41-4945-9392-ac694cb71afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new ThresholdTunedClassifier object\n",
    "logit = ThresholdTunedClassifier()\n",
    "\n",
    "# Perform hyperparamater tuning\n",
    "logit_base = LogisticRegression(max_iter=1000, class_weight=\"balanced\", solver=\"saga\", random_state=123)\n",
    "logit_param_grid = {\"C\": np.logspace(-3, 3, 13), \"penalty\": [\"l1\", \"l2\"]}\n",
    "logit.model_tuning(X_train, y_train, logit_base, logit_param_grid, n_iter=1, n_folds=3, tuner_scoring=\"average_precision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d020bd2b-6107-42c1-9a11-39ad564182ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune Decision Threshold\n",
    "logit_oof_proba, logit_thr_raw, logit_f1_raw = logit.decision_thr_tuning(X_train, y_train, n_folds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2760b899-aa54-4eea-9889-c7e1c4a07984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibrate the model\n",
    "logit_thr_cal = logit.calibrate_prob(y_train, logit_oof_proba, logit_thr_raw, logit_f1_raw, calibration_method='isotonic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bce6c1a-d10d-4064-876b-cdf2205528eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the optimal model with the best set of hyper-parameters\n",
    "logit.model_training(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd99dd64-0d6b-47e2-9c96-239bdcb2d5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze model performance\n",
    "logit.analyze_performance(X_train, y_train, n_folds=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5db1c09-8794-4a7d-ab71-3f8a913e2b23",
   "metadata": {
    "tags": []
   },
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36b607c-1e05-4d0e-a54f-6636884d8806",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform hyper-parameter tuning using Optunna\n",
    "def xgb_objective(trial):\n",
    "    # define model hyper-parameters and hyper-parameter search space\n",
    "    scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum() # From XGBoost documentation: the weight should be the ratio between the negative class and postiive class, not the other way around\n",
    "    params = {\n",
    "        \"objective\": 'binary:logistic',\n",
    "        \"eval_metric\": [\"aucpr\"], # This has to be defined, because this is the metric that will be used during pruning\n",
    "        \"random_state\": 98464,\n",
    "        \"scale_pos_weight\": scale_pos_weight,\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 3e-1, log=True),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 300, 1000, step=100),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 1.0, 32.0, log=True),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0.0, 1.0),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"colsample_bynode\": trial.suggest_float(\"colsample_bynode\", 0.7, 1.0),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 1e+1, log=True),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 1e+1, log=True)\n",
    "    }\n",
    "    \n",
    "    model = XGBClassifier(**params)\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=67)\n",
    "    scores = []\n",
    "\n",
    "    for train_idx, val_idx in cv.split(X_train, y_train):\n",
    "        pruning_cb = XGBoostPruningCallback(trial, \"validation_0-aucpr\") # Boosting-round based pruning\n",
    "        model.fit(\n",
    "            X_train.iloc[train_idx,:], y_train.iloc[train_idx],\n",
    "            eval_set=[(X_train.iloc[val_idx,:], y_train.iloc[val_idx])],\n",
    "            callbacks=[pruning_cb],          # pruning each boosting round\n",
    "            verbose=False\n",
    "        )\n",
    "        proba = model.predict_proba(X_train.iloc[val_idx,:])[:, 1]\n",
    "        scores.append(average_precision_score(y_train.iloc[val_idx], proba))\n",
    "\n",
    "    return float(np.mean(scores))\n",
    "\n",
    "# Define the pruner\n",
    "pruner = optuna.pruners.SuccessiveHalvingPruner(\n",
    "    min_resource=1,          # start checking at first fold\n",
    "    reduction_factor=3,      # roughly keep top 1/3 at each rung\n",
    "    min_early_stopping_rate=0\n",
    ")\n",
    "\n",
    "# Perform hyper-parameter tuning\n",
    "xgb_study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    sampler=optuna.samplers.TPESampler(seed=2025, n_startup_trials=20),\n",
    "    pruner=pruner\n",
    ")\n",
    "xgb_study.optimize(xgb_objective, n_trials=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f59088d-5683-4840-b222-bdbebd8c60e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best AP:\", xgb_study.best_value)\n",
    "print(\"Best params:\", xgb_study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cebf262-5b92-413c-8302-7639954cac4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new ThresholdTunedClassifier object\n",
    "xgb = ThresholdTunedClassifier()\n",
    "\n",
    "# Define the optimized model\n",
    "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum() # From XGBoost documentation: the weight should be the ratio between the negative class and postiive class, not the other way around\n",
    "xgb.enter_best_model_config(XGBClassifier(\n",
    "    objective = 'binary:logistic',\n",
    "    eval_metric = [\"aucpr\"], # This has to be defined, because this is the metric that will be used during pruning\n",
    "    random_state = 98464,\n",
    "    scale_pos_weight = scale_pos_weight,\n",
    "    **xgb_study.best_params\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21580f70-930d-4a22-a8ed-6c5dd71ddcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune Decision Threshold\n",
    "xgb_oof_proba, xgb_thr_raw, xgb_f1_raw = xgb.decision_thr_tuning(X_train, y_train, n_folds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855db553-08e8-4609-92e4-e6ff07ab2176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibrate the model\n",
    "xgb_thr_cal = xgb.calibrate_prob(y_train, xgb_oof_proba, xgb_thr_raw, xgb_f1_raw, calibration_method='isotonic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77c7f93-7ff0-44a7-9679-f33e2a184060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the optimal model with the best set of hyper-parameters\n",
    "xgb.model_training(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b1cbbe-5bb5-4513-80f1-287757c63597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze model performance\n",
    "xgb.analyze_performance(X_train, y_train, n_folds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5faabad-3393-4a9c-9bae-2d6339f4bf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform probability OOF prediction on training dataset\n",
    "xgb_train_proba_pred = xgb.generate_oof_proba_preds(X_train, y_train, n_folds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dda19c-4a8d-4551-a97d-fa228f3fbdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform probability prediction on testing dataset\n",
    "xgb_test_proba_pred = xgb.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f98117-51ec-46ae-9fbb-fde6fb905640",
   "metadata": {
    "tags": []
   },
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92ecd90-6ee0-4fb6-b707-82238e32b274",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform hyper-parameter tuning using Optunna\n",
    "def lgbm_objective(trial):\n",
    "    # define model hyper-parameters and hyper-parameter search space\n",
    "    scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum() # From XGBoost documentation: the weight should be the ratio between the negative class and postiive class, not the other way around\n",
    "    \n",
    "    boosting_type = trial.suggest_categorical(\"boosting_type\", [\"gbdt\", \"goss\"])\n",
    "    \n",
    "    cond_params = {}\n",
    "    if boosting_type == \"gbdt\":\n",
    "        cond_params[\"subsample\"] = trial.suggest_float(\"subsample\", 0.7, 1.0)\n",
    "        cond_params[\"bagging_freq\"] = trial.suggest_int(\"bagging_freq\", 1, 5)\n",
    "    elif boosting_type == \"goss\":\n",
    "        top_rate = trial.suggest_float(\"top_rate\", 0.1, 0.5)\n",
    "        other_rate = trial.suggest_float(\"other_rate\", 0.1, 0.5)\n",
    "        # Ensure top_rate + other_rate < 1\n",
    "        other_rate = min(other_rate, 0.99-top_rate)\n",
    "        cond_params[\"top_rate\"] = top_rate\n",
    "        cond_params[\"other_rate\"] = other_rate\n",
    "    \n",
    "    params = {\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"average_precision\", # ensures LGBM logs AP, as it is the metric monitored during Optuna's pruning\n",
    "        \"random_state\": 42,\n",
    "        \"scale_pos_weight\": scale_pos_weight,\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.2, log=True),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 600, 1800, step=100),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 31, 500, log=True),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", -1, 12), # max_depth=-1 lets LGBM cap by num_leaves.\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 10, 200),\n",
    "        \"min_sum_hessian_in_leaf\": trial.suggest_float(\"min_sum_hessian_in_leaf\", 1e-3, 10.0, log=True),\n",
    "        \"min_split_gain\": trial.suggest_float(\"min_split_gain\", 0.0, 0.5),\n",
    "        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n",
    "        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n",
    "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.6, 1.0),\n",
    "        \"feature_fraction_bynode\": trial.suggest_float(\"feature_fraction_bynode\", 0.6, 1.0),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 63, 300, step=32),\n",
    "        **cond_params # Append the conditional hyper-parameters\n",
    "    }\n",
    "    \n",
    "    model = LGBMClassifier(**params)\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=67)\n",
    "    scores = []\n",
    "\n",
    "    for train_idx, val_idx in cv.split(X_train, y_train):\n",
    "        pruning_cb = LightGBMPruningCallback(trial, \"average_precision\") # Boosting-round based pruning\n",
    "        model.fit(\n",
    "            X_train.iloc[train_idx,:], y_train.iloc[train_idx],\n",
    "            eval_set=[(X_train.iloc[val_idx,:], y_train.iloc[val_idx])],\n",
    "            callbacks=[pruning_cb]          # pruning each boosting round\n",
    "        )\n",
    "        proba = model.predict_proba(X_train.iloc[val_idx,:])[:, 1]\n",
    "        scores.append(average_precision_score(y_train.iloc[val_idx], proba))\n",
    "\n",
    "    return float(np.mean(scores))\n",
    "\n",
    "# Define the pruner\n",
    "pruner = optuna.pruners.SuccessiveHalvingPruner(\n",
    "    min_resource=1,          # start checking at first fold\n",
    "    reduction_factor=3,      # roughly keep top 1/3 at each rung\n",
    "    min_early_stopping_rate=0\n",
    ")\n",
    "\n",
    "# Perform hyper-parameter tuning\n",
    "lgbm_study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    sampler=optuna.samplers.TPESampler(seed=2025, n_startup_trials=20),\n",
    "    pruner=pruner\n",
    ")\n",
    "lgbm_study.optimize(lgbm_objective, n_trials=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eedaf87-2ee5-4405-a902-6ada7888ec83",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best AP:\", lgbm_study.best_value)\n",
    "print(\"Best params:\", lgbm_study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a2c090-b61b-44cf-8e40-b18e0b7d5800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new ThresholdTunedClassifier object\n",
    "lgbm = ThresholdTunedClassifier()\n",
    "\n",
    "# Define the optimized model\n",
    "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "lgbm.enter_best_model_config(LGBMClassifier(\n",
    "    objective = \"binary\",\n",
    "    metric = \"average_precision\",\n",
    "    random_state = 42,\n",
    "    scale_pos_weight = scale_pos_weight,\n",
    "    **lgbm_study.best_params\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead2ce09-b97f-4eac-ab4b-5818a7f415bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune Decision Threshold\n",
    "lgbm_oof_proba, lgbm_thr_raw, lgbm_f1_raw = lgbm.decision_thr_tuning(X_train, y_train, n_folds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429a4e59-8276-4f71-9586-0d4a86a6d437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibrate the model\n",
    "lgbm_thr_cal = lgbm.calibrate_prob(y_train, lgbm_oof_proba, lgbm_thr_raw, lgbm_f1_raw, calibration_method='isotonic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a6900f-de55-4800-b782-4fc0e36efddf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train the optimal model with the best set of hyper-parameters\n",
    "lgbm.model_training(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452e8e59-94b5-44cb-96f7-de8df8794e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze model performance\n",
    "lgbm.analyze_performance(X_train, y_train, n_folds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6d5b42-8ed5-4b95-83a2-193cb7425216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform probability OOF prediction on training dataset\n",
    "lgbm_train_proba_pred = lgbm.generate_oof_proba_preds(X_train, y_train, n_folds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036922d1-10aa-4245-8467-b0424da6e5af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform probability prediction on testing dataset\n",
    "lgbm_test_proba_pred = lgbm.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e592f7db-89c4-4eb1-85aa-a5886bfb3980",
   "metadata": {
    "tags": []
   },
   "source": [
    "## CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77686c84-7f50-43df-b98d-617b17fa64fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pre-transformed categorical features (e.g. categorical features not encoded)\n",
    "X_train_catb = df_train.copy().drop('loan_status', axis=1)\n",
    "X_test_catb = df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1d9129-ee0a-440f-bb2d-01c1e596b22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical columns (i.e. of type \"object\" or \"category\")\n",
    "catb_cols = X_train_catb.select_dtypes(include=['object', 'category']).columns.to_list()\n",
    "catb_idx = [X_train_catb.columns.get_loc(col) for col in catb_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0310625-79a0-4fa4-bc59-a5b23d310bca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform hyper-parameter tuning using Optunna\n",
    "def catb_objective(trial):\n",
    "    # define model hyper-parameters and hyper-parameter search space\n",
    "    scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "    class_weights = {0: 1.0, 1: scale_pos_weight}\n",
    "    \n",
    "    params = {\n",
    "        \"loss_function\": 'Logloss',\n",
    "        \"eval_metric\": 'PRAUC',           # Precision-Recall AUC\n",
    "        \"grow_policy\": 'SymmetricTree',    # Default\n",
    "        \"bootstrap_type\": 'Bayesian',     # Default\n",
    "        \"cat_features\": catb_idx,          # list of categorical column indices\n",
    "        \"random_seed\": 42,\n",
    "        \"class_weights\": class_weights,\n",
    "        'iterations': trial.suggest_int(\"iterations\", 600, 1800, step=100),\n",
    "        'learning_rate': trial.suggest_float(\"learning_rate\", 1e-3, 0.3, log=True),\n",
    "        'depth': trial.suggest_int(\"depth\", 4, 10),\n",
    "        'l2_leaf_reg': trial.suggest_float(\"l2_leaf_reg\", 1e-2, 50.0, log=True),\n",
    "        'random_strength': trial.suggest_float(\"random_strength\", 0.0, 2.0),\n",
    "        'bagging_temperature': trial.suggest_float(\"bagging_temperature\", 0.0, 5.0),   # for Bayesian bootstrap\n",
    "        'colsample_bylevel': trial.suggest_float(\"colsample_bylevel\", 0.6, 1.0),\n",
    "        'border_count': trial.suggest_int(\"border_count\", 64, 255, step=32),\n",
    "        'one_hot_max_size': trial.suggest_int(\"one_hot_max_size\", 2, 10),  # threshold for OHE vs Ordered CTR\n",
    "        'max_ctr_complexity': trial.suggest_int(\"max_ctr_complexity\", 1, 2) # Ordered Combo CTR depth\n",
    "    }\n",
    "    \n",
    "    \n",
    "    model = CatBoostClassifier(**params)\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=67)\n",
    "    scores = []\n",
    "\n",
    "    for train_idx, val_idx in cv.split(X_train_catb, y_train):\n",
    "        pruning_cb = CatBoostPruningCallback(trial, \"PRAUC\") # Boosting-round based pruning\n",
    "        model.fit(\n",
    "            X_train_catb.iloc[train_idx,:], y_train.iloc[train_idx],\n",
    "            eval_set=[(X_train_catb.iloc[val_idx,:], y_train.iloc[val_idx])],\n",
    "            callbacks=[pruning_cb]          # pruning each boosting round\n",
    "        )\n",
    "        proba = model.predict_proba(X_train_catb.iloc[val_idx,:])[:, 1]\n",
    "        scores.append(average_precision_score(y_train.iloc[val_idx], proba))\n",
    "\n",
    "    return float(np.mean(scores))\n",
    "\n",
    "# Define the pruner\n",
    "pruner = optuna.pruners.SuccessiveHalvingPruner(\n",
    "    min_resource=1,          # start checking at first fold\n",
    "    reduction_factor=3,      # roughly keep top 1/3 at each rung\n",
    "    min_early_stopping_rate=0\n",
    ")\n",
    "\n",
    "# Perform hyper-parameter tuning\n",
    "catb_study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    sampler=optuna.samplers.TPESampler(seed=2025, n_startup_trials=20),\n",
    "    pruner=pruner\n",
    ")\n",
    "catb_study.optimize(catb_objective, n_trials=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db14d963-5fb8-4313-b187-b70c586f9b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best AP:\", catb_study.best_value)\n",
    "print(\"Best params:\", catb_study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5099dc4-d21e-4d1e-b08b-206d25815a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new ThresholdTunedClassifier object\n",
    "catb = ThresholdTunedClassifier()\n",
    "\n",
    "# Define the optimized model\n",
    "# define model hyper-parameters and hyper-parameter search space\n",
    "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "class_weights = {0: 1.0, 1: scale_pos_weight}\n",
    "catb.enter_best_model_config(CatBoostClassifier(\n",
    "    loss_function = 'Logloss',\n",
    "    eval_metric = 'PRAUC',\n",
    "    grow_policy = 'SymmetricTree',\n",
    "    bootstrap_type = 'Bayesian',\n",
    "    cat_features = catb_idx,\n",
    "    random_seed = 42,\n",
    "    class_weights = class_weights,\n",
    "    **catb_study.best_params\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d24183b-ff94-4d98-b7e3-1c7db004e2aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tune Decision Threshold\n",
    "catb_oof_proba, catb_thr_raw, catb_f1_raw = catb.decision_thr_tuning(X_train_catb, y_train, n_folds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99fd9c2-813a-4303-8b61-189a5081e529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibrate the model\n",
    "catb_thr_cal = catb.calibrate_prob(y_train, catb_oof_proba, catb_thr_raw, catb_f1_raw, calibration_method='isotonic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a30311-b4d6-40c5-b199-6a554c305545",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train the optimal model with the best set of hyper-parameters\n",
    "catb.model_training(X_train_catb, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62eb1c71-8fc9-444a-b887-d3a35201e200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze model performance\n",
    "catb.analyze_performance(X_train_catb, y_train, n_folds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f58f2c-b3a9-451c-bdcc-8ab2342a770e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform probability OOF prediction on training dataset\n",
    "catb_train_proba_pred = catb.generate_oof_proba_preds(X_train_catb, y_train, n_folds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b195baf-6f84-4697-ae3c-25aa4df0d8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform probability prediction\n",
    "catb_test_proba_pred = catb.predict_proba(X_test_catb)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fe1049-d61e-449e-860e-36da00ace5f7",
   "metadata": {},
   "source": [
    "- Do I perform one-hot-encoding on the Categorical features? Or should I just leave them as it is?\n",
    "- Do CatBoost treat ordinal and nominal categorical features the same way?\n",
    "- Apply class weights during training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322b9e4f-cd04-42be-a382-879af81e0b8c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc5df81-2c4a-4986-a63a-9fb3e4e76962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine probability predicitons of the base models on trianing dataset\n",
    "X_train_stacking = np.stack([xgb_train_proba_pred, lgbm_train_proba_pred, catb_train_proba_pred], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7646a8-1206-4bd0-8d13-317ac89ad0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new ThresholdTunedClassifier object\n",
    "stacking_model = ThresholdTunedClassifier()\n",
    "\n",
    "# Perform hyperparamater tuning\n",
    "stacking_base = LogisticRegression(max_iter=1000, class_weight=\"balanced\", solver=\"saga\", random_state=123)\n",
    "stacking_param_grid = {\"C\": np.logspace(-3, 3, 13), \"penalty\": [\"l1\", \"l2\"]}\n",
    "stacking_model.model_tuning(X_train_stacking, y_train, stacking_base, stacking_param_grid, n_iter=20, n_folds=3, tuner_scoring=\"average_precision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c0151e-728a-47ad-9b93-8180fa6d2ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune Decision Threshold\n",
    "stacking_oof_proba, stacking_thr_raw, stacking_f1_raw = stacking_model.decision_thr_tuning(X_train_stacking, y_train, n_folds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754ece90-f283-4ee3-ab0a-f4b9a1795555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibrate the model\n",
    "stacking_thr_cal = stacking_model.calibrate_prob(y_train, stacking_oof_proba, stacking_thr_raw, stacking_f1_raw, calibration_method='isotonic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb29f903-6daa-4e6b-b05f-ee4674032e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the optimal model with the best set of hyper-parameters\n",
    "stacking_model.model_training(X_train_stacking, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f262e44-b901-4f16-81c2-86b46f776d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze model performance\n",
    "stacking_model.analyze_performance(X_train_stacking, y_train, n_folds=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faad1618-4929-41c7-9c1d-a9f55529ef5b",
   "metadata": {},
   "source": [
    "## Prediction on Testing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97bfc2d-b1c3-45e2-a28e-da999b068142",
   "metadata": {},
   "source": [
    "Generate prediciton on testing dataset using the stacking model. Ready for submission!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc600a5-ac4d-4703-89a6-31a38992751a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine probability predicitons of the base models on testing dataset\n",
    "X_test_stacking = np.stack([xgb_test_proba_pred, lgbm_test_proba_pred, catb_test_proba_pred], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9354cf3-53d6-44be-bf94-378917c18d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class prediction on testing dataset\n",
    "stacking_model.predict(X_test_stacking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44f7d6f-2bea-4b93-9efe-498ad36f2601",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
